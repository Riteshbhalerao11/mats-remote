{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55444c7e-e6ec-4d7e-80f0-8e2893fdef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from nnsight import LanguageModel\n",
    "from nnsight.intervention.envoy import Envoy\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import textwrap\n",
    "from typing import List, Optional, Union, Tuple, Literal, Any, cast\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "from utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce4311f-979d-4c67-b1a7-7b8b23bfc8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SteeringArgs:\n",
    "    # === Model & Data Paths ===\n",
    "    model: str = \"/pscratch/sd/r/ritesh11/temp/Qwen3-30B-A3B/\"\n",
    "    vec_dir: str = \"steering_vectors/\"\n",
    "    prompts_dir: str = \"/pscratch/sd/r/ritesh11/temp/steering_experiments/drive_repo/steering_test_yamls\"\n",
    "    res_dir: str = 'steered_outs'\n",
    "    mode: str = \"eval\"  # choices: \"eval\" or \"deploy\"\n",
    "\n",
    "    # === Data Type & Randomness ===\n",
    "    dtype: str = \"auto\"  # choices: \"bfloat16\", \"float16\", \"float32\"\n",
    "    random_seed: int = 42\n",
    "\n",
    "    # === Generation Settings ===\n",
    "    batch_size: int = 4\n",
    "    max_new_tokens: int = 2000\n",
    "    temperature: float = 0.6\n",
    "    top_p: float = 0.95\n",
    "\n",
    "    # === Steering Configuration ===\n",
    "    d_model: int = 2048\n",
    "    model_len: int = 28\n",
    "    steer_on_user: bool = True\n",
    "    steer_on_thinking: bool = True\n",
    "    steer_on_system: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5878c77f-5551-41aa-96d8-f0a5181c35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SteeringArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f71df4-2a56-4c21-a276-7422e7b8ce8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad743578ca04975be09168f37999d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    args.model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=args.dtype,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
    "\n",
    "# Wrap with nnsight\n",
    "nnmodel = LanguageModel(model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca59151-2a2d-4141-bffa-e46ae095d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steer_and_generate(\n",
    "    prompt_list: List[str],\n",
    "    system_prompt: Union[str, List[str]],\n",
    "    lma,\n",
    "    tokenizer,\n",
    "    steering_vectors: dict[torch.Tensor, float] | None,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float,\n",
    "    layer_to_steer: int | Literal['all'] | List[int],\n",
    "    d_model: int,\n",
    "    steer_on_user: bool,\n",
    "    steer_on_thinking: bool,\n",
    "    steer_on_system: bool,\n",
    "    top_p: float,\n",
    "    resdir: str,\n",
    "    filenames: List[str],\n",
    "    backup_every: int = 1,\n",
    ") -> Tuple[List[str], List[str], List[Any], List[Any]]:\n",
    "    \"\"\"\n",
    "    Generate steered responses for a list of prompts with optional user-token-only steering.\n",
    "    \n",
    "    Args:\n",
    "        prompt_list: List of prompts to process\n",
    "        lma: LanguageModel instance\n",
    "        tokenizer: AutoTokenizer instance\n",
    "        steering_vectors: Dict mapping tensors to their multipliers (default: None)\n",
    "        batch_size: Number of prompts to process in each batch\n",
    "        max_new_tokens: Maximum number of new tokens to generate\n",
    "        temperature: Temperature for generation\n",
    "        layer_to_steer: Layer(s) to apply steering to\n",
    "        d_model: Model dimension\n",
    "        system_prompt: System prompt(s) to use - can be a single string or list of strings\n",
    "        steer_on_user: Whether to steer on user prompt tokens\n",
    "        steer_on_thinking: Whether to steer on thinking tokens\n",
    "        steer_on_system: Whether to steer on system prompt tokens\n",
    "        top_p: Nucleus sampling parameter\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (full_responses, model_only_responses, tok_batches, out_steered_list)\n",
    "        - full_responses: Complete decoded outputs including prompts\n",
    "        - model_only_responses: Only the generated parts (excluding input prompts)\n",
    "        - tok_batches: List of tokenized batches\n",
    "        - out_steered_list: List of raw output tensors\n",
    "\n",
    "    Example steering vector: {difference_vector: 0.5}\n",
    "    \"\"\"\n",
    "    os.makedirs(resdir, exist_ok=True)\n",
    "    \n",
    "    layers, model_len, is_ft, embed = get_model_info(lma)\n",
    "    total_steering, steering_vec_list = prepare_steering_vectors(\n",
    "        steering_vectors, layer_to_steer, d_model, model_len\n",
    "    )\n",
    "\n",
    "    # Format prompts with chat template\n",
    "    formatted_string_list = []\n",
    "    # Handle single system prompt vs list of system prompts\n",
    "    if isinstance(system_prompt, str):\n",
    "        # Single system prompt for all\n",
    "        for p in prompt_list:\n",
    "            question_string = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": p}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_string_list.append(question_string)\n",
    "    else:\n",
    "        # Different system prompts - should have one per prompt\n",
    "        assert len(system_prompt) == len(prompt_list), f\"System prompt list length {len(system_prompt)} doesn't match prompt list length {len(prompt_list)}\"\n",
    "        for p, sys_p in zip(prompt_list, system_prompt):\n",
    "            question_string = tokenizer.apply_chat_template(\n",
    "                conversation=[\n",
    "                    {\"role\": \"system\", \"content\": sys_p},\n",
    "                    {\"role\": \"user\", \"content\": p}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_string_list.append(question_string)\n",
    "    \n",
    "    # Create batches\n",
    "    tok_batches = []\n",
    "    prompt_batches = []\n",
    "    system_prompt_batches: List[Union[str, List[str]]] = []  # Track system prompts for each batch\n",
    "    batch_indices_list = []  # Track original indices for each batch\n",
    "    \n",
    "    for i in range(0, len(formatted_string_list), batch_size):\n",
    "        batch_strings = formatted_string_list[i:i+batch_size]\n",
    "        batch_prompts = prompt_list[i:i+batch_size]\n",
    "        batch_indices = list(range(i, min(i+batch_size, len(prompt_list))))\n",
    "        \n",
    "        # Get system prompts for this batch\n",
    "        batch_system_prompts: Union[str, List[str]]\n",
    "        if isinstance(system_prompt, str):\n",
    "            batch_system_prompts = system_prompt\n",
    "        else:\n",
    "            batch_system_prompts = system_prompt[i:i+batch_size]\n",
    "        \n",
    "        tok_batch = tokenizer(\n",
    "            batch_strings, \n",
    "            add_special_tokens=False, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True,\n",
    "            padding_side=\"left\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        tok_batches.append(tok_batch)\n",
    "        prompt_batches.append(batch_prompts)\n",
    "        system_prompt_batches.append(batch_system_prompts)\n",
    "        batch_indices_list.append(batch_indices)\n",
    "    # print tokenization to make sure it's right, also check the size of the masks, also steer on only one layer. \n",
    "    full_responses = []\n",
    "    model_only_responses = []\n",
    "    out_steered_list = []\n",
    "    \n",
    "    for b_idx, (tok_batch, prompt_batch, sys_prompt_batch, batch_indices) in enumerate(\n",
    "        tqdm(zip(tok_batches, prompt_batches, system_prompt_batches, batch_indices_list), total=len(tok_batches))):\n",
    "        \n",
    "        # Create user token mask if steering is enabled and user-only steering is requested\n",
    "        user_mask = None\n",
    "        if steering_vectors is not None and steer_on_user:\n",
    "            user_mask = create_user_token_mask(prompt_batch, tok_batch, tokenizer)\n",
    "        \n",
    "        # Create system token mask if steering is enabled and system-only steering is requested\n",
    "        system_mask = None\n",
    "        if steering_vectors is not None and steer_on_system:\n",
    "            # Pass batch indices if we have a list of system prompts\n",
    "            if isinstance(system_prompt, list):\n",
    "                system_mask = create_system_token_mask(sys_prompt_batch, tok_batch, tokenizer)\n",
    "            else:\n",
    "                system_mask = create_system_token_mask(sys_prompt_batch, tok_batch, tokenizer)\n",
    "        \n",
    "        # Generate with or without steering\n",
    "        if steering_vectors is None:\n",
    "            with lma.generate(tok_batch, max_new_tokens=max_new_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                out_steered = lma.generator.output.save()\n",
    "        elif (steer_on_user or steer_on_system) and not steer_on_thinking:\n",
    "            steering_vec_list = cast(List[torch.Tensor], steering_vec_list)\n",
    "            \n",
    "            # Create combined mask for user and/or system tokens\n",
    "            combined_mask = None\n",
    "            if steer_on_user and steer_on_system:\n",
    "                user_mask = cast(torch.Tensor, user_mask)\n",
    "                system_mask = cast(torch.Tensor, system_mask)\n",
    "                combined_mask = torch.logical_or(user_mask, system_mask)\n",
    "            elif steer_on_user:\n",
    "                combined_mask = cast(torch.Tensor, user_mask)\n",
    "            elif steer_on_system:\n",
    "                combined_mask = cast(torch.Tensor, system_mask)\n",
    "            \n",
    "            # Simple case: only steer on user/system tokens, no thinking steering\n",
    "            with lma.generate(tok_batch, max_new_tokens=max_new_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                # Apply steering to user/system tokens only at the beginning\n",
    "                if layer_to_steer == 'all':\n",
    "                    for i in range(model_len):\n",
    "                        apply_steering_to_layer(layers[i], steering_vec_list[i], combined_mask)\n",
    "                elif isinstance(layer_to_steer, list):\n",
    "                    for i in layer_to_steer:\n",
    "                        apply_steering_to_layer(layers[i], steering_vec_list[i], combined_mask)\n",
    "                else:  # Single layer\n",
    "                    apply_steering_to_layer(layers[layer_to_steer], total_steering, combined_mask)\n",
    "                \n",
    "                out_steered = lma.generator.output.save()\n",
    "        else:\n",
    "            steering_vec_list = cast(List[torch.Tensor], steering_vec_list)\n",
    "            \n",
    "            # Create initial combined mask for user and/or system tokens\n",
    "            initial_mask = None\n",
    "            if steer_on_user and steer_on_system:\n",
    "                user_mask = cast(torch.Tensor, user_mask)\n",
    "                system_mask = cast(torch.Tensor, system_mask)\n",
    "                initial_mask = torch.logical_or(user_mask, system_mask)\n",
    "            elif steer_on_user:\n",
    "                initial_mask = cast(torch.Tensor, user_mask)\n",
    "            elif steer_on_system:\n",
    "                initial_mask = cast(torch.Tensor, system_mask)\n",
    "            \n",
    "            # Complex case: need thinking steering (with or without user/system steering)\n",
    "            # create a mask for which batch position haven't\n",
    "            # gotten like the first sentence of thinking tokens done yet.\n",
    "            mask_for_first_period = torch.zeros(tok_batch['input_ids'].shape[0], dtype = torch.bool, device = \"cuda\")\n",
    "\n",
    "            # First generation phase - generate up to 150 tokens with original logic\n",
    "            max_phase1_tokens = min(150, max_new_tokens)\n",
    "            \n",
    "            with lma.generate(tok_batch, max_new_tokens=max_phase1_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                # Apply steering to specified layers\n",
    "                for j in range(max_phase1_tokens):\n",
    "                    if j == 0:\n",
    "                        if steer_on_user or steer_on_system:\n",
    "                            if layer_to_steer == 'all':\n",
    "                                for i in range(model_len):\n",
    "                                    apply_steering_to_layer(layers[i], steering_vec_list[i], initial_mask)\n",
    "                                    layers[i].next()\n",
    "                                        \n",
    "                            elif isinstance(layer_to_steer, list):\n",
    "                                for i in layer_to_steer:\n",
    "                                    apply_steering_to_layer(layers[i], steering_vec_list[i], initial_mask)\n",
    "                                    layers[i].next()\n",
    "                            else:  # Single layer\n",
    "                                apply_steering_to_layer(layers[layer_to_steer], total_steering, initial_mask)\n",
    "                                layers[layer_to_steer].next()\n",
    "                        else:\n",
    "                            # No user/system steering, just call next\n",
    "                            if layer_to_steer == 'all':\n",
    "                                for i in range(model_len):\n",
    "                                    layers[i].next()  \n",
    "                            elif isinstance(layer_to_steer, list):\n",
    "                                for i in layer_to_steer:\n",
    "                                    layers[i].next()\n",
    "                            else:  # Single layer\n",
    "                                layers[layer_to_steer].next()\n",
    "\n",
    "                    else:\n",
    "                        if steer_on_thinking:\n",
    "                            #update mask\n",
    "                            cur_period = embed.input.squeeze() == 13\n",
    "                            # assert embed.input.shape[0] == mask_for_first_period.shape[0], f\"Batch size mismatch: {embed.input.shape[0]} != {mask_for_first_period.shape[0]}\"\n",
    "                            # assert embed.input.shape[1] == 1\n",
    "                            mask_for_first_period = torch.logical_or(cur_period, mask_for_first_period.detach())\n",
    "                            #go through each layer, steer, then call next\n",
    "                            if layer_to_steer == 'all':\n",
    "                                for i in range(model_len):\n",
    "                                    apply_steering_to_layer(layers[i], steering_vec_list[i], mask_for_first_period.unsqueeze(-1))\n",
    "                                    layers[i].next()\n",
    "                            elif isinstance(layer_to_steer, list):\n",
    "                                for i in layer_to_steer:\n",
    "                                    apply_steering_to_layer(layers[i], steering_vec_list[i], mask_for_first_period.unsqueeze(-1))\n",
    "                                    layers[i].next()\n",
    "                            else:  # Single layer\n",
    "                                apply_steering_to_layer(layers[layer_to_steer], total_steering, mask_for_first_period.unsqueeze(-1))\n",
    "                                layers[layer_to_steer].next()\n",
    "                    embed.next()\n",
    "                \n",
    "                phase1_output = lma.generator.output.save()\n",
    "            \n",
    "            # Check if we need to continue generation\n",
    "            remaining_tokens = max_new_tokens - max_phase1_tokens\n",
    "            \n",
    "            if remaining_tokens > 0:\n",
    "                # Find where we've seen periods in phase 1\n",
    "                batch_size = phase1_output.shape[0]\n",
    "                period_token_id = 13\n",
    "                \n",
    "                # Create mask for phase 2 - need to track which positions had periods\n",
    "                phase2_length = phase1_output.shape[1]\n",
    "                phase2_mask_for_first_period = torch.zeros((batch_size, phase2_length), dtype=torch.bool, device=\"cuda\")\n",
    "                \n",
    "                # Find positions after first period for each sequence\n",
    "                for b in range(batch_size):\n",
    "                    period_positions = (phase1_output[b] == period_token_id).nonzero(as_tuple=True)[0]\n",
    "                    if len(period_positions) > 0:\n",
    "                        first_period_pos = period_positions[0].item()\n",
    "                        phase2_mask_for_first_period[b, first_period_pos + 1:] = True\n",
    "                \n",
    "                # Create attention mask for phase 2\n",
    "                phase2_attention_mask = torch.ones_like(phase1_output, dtype=torch.long, device=\"cuda\")\n",
    "                if 'attention_mask' in tok_batch:\n",
    "                    orig_mask_length = tok_batch['attention_mask'].shape[1]\n",
    "                    phase2_attention_mask[:, :orig_mask_length] = tok_batch['attention_mask']\n",
    "                \n",
    "                # Continue generation with phase 2\n",
    "                phase2_input = {\n",
    "                    'input_ids': phase1_output,\n",
    "                    'attention_mask': phase2_attention_mask\n",
    "                }\n",
    "                \n",
    "                with lma.generate(phase2_input, max_new_tokens=remaining_tokens, temperature=temperature, pad_token_id=tokenizer.pad_token_id, top_p=top_p) as gen:\n",
    "                    # Continue with the same pattern using .next()\n",
    "                    for i in range(remaining_tokens):\n",
    "                        if i == 0:\n",
    "                            # Apply initial steering to the existing sequence based on our masks\n",
    "                            if (steer_on_user or steer_on_system) and initial_mask is not None:\n",
    "                                # Create combined mask for initial steering\n",
    "                                combined_initial_mask = torch.zeros((batch_size, phase2_length), dtype=torch.bool, device=\"cuda\")\n",
    "                                initial_mask_length = initial_mask.shape[1]\n",
    "                                combined_initial_mask[:, :initial_mask_length] = initial_mask\n",
    "                                combined_initial_mask = torch.logical_or(combined_initial_mask, phase2_mask_for_first_period)\n",
    "                                \n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for l in range(model_len):\n",
    "                                        apply_steering_to_layer(layers[l], steering_vec_list[l], combined_initial_mask)\n",
    "                                        layers[l].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for l in layer_to_steer:\n",
    "                                        apply_steering_to_layer(layers[l], steering_vec_list[l], combined_initial_mask)\n",
    "                                        layers[l].next()\n",
    "                                else:\n",
    "                                    apply_steering_to_layer(layers[layer_to_steer], total_steering, combined_initial_mask)\n",
    "                                    layers[layer_to_steer].next()\n",
    "                            else:\n",
    "                                # Just apply thinking steering\n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for l in range(model_len):\n",
    "                                        apply_steering_to_layer(layers[l], steering_vec_list[l], phase2_mask_for_first_period)\n",
    "                                        layers[l].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for l in layer_to_steer:\n",
    "                                        apply_steering_to_layer(layers[l], steering_vec_list[l], phase2_mask_for_first_period)\n",
    "                                        layers[l].next()\n",
    "                                else:\n",
    "                                    apply_steering_to_layer(layers[layer_to_steer], total_steering, phase2_mask_for_first_period)\n",
    "                                    layers[layer_to_steer].next()\n",
    "                        else:\n",
    "                            # For subsequent tokens, apply thinking steering to all\n",
    "                            if steer_on_thinking:\n",
    "                                if layer_to_steer == 'all':\n",
    "                                    for l in range(model_len):\n",
    "                                        layers[l].output[:,:,:] = layers[l].output[:,:,:] + steering_vec_list[l].unsqueeze(0).unsqueeze(0)\n",
    "                                        layers[l].next()\n",
    "                                elif isinstance(layer_to_steer, list):\n",
    "                                    for l in layer_to_steer:\n",
    "                                        layers[l].output[:,:,:] = layers[l].output[:,:,:] + steering_vec_list[l].unsqueeze(0).unsqueeze(0)\n",
    "                                        layers[l].next()\n",
    "                                else:\n",
    "                                    layers[layer_to_steer].output[0][:,:,:] = layers[layer_to_steer].output[0][:,:,:] + total_steering.unsqueeze(0).unsqueeze(0)\n",
    "                                    layers[layer_to_steer].next()\n",
    "                        embed.next()\n",
    "                    \n",
    "                    out_steered = lma.generator.output.save()\n",
    "            else:\n",
    "                out_steered = phase1_output\n",
    "        \n",
    "        out_steered_list.append(out_steered)\n",
    "        \n",
    "        # Decode responses\n",
    "        full_decoded = tokenizer.batch_decode(out_steered)\n",
    "        full_decoded = [d.replace(tokenizer.eos_token, '').replace('<|end_of_text|>', '') for d in full_decoded]\n",
    "        full_responses.extend(full_decoded)\n",
    "        \n",
    "        # Decode model-only responses (excluding input prompts)\n",
    "        model_only_decoded = []\n",
    "        for i, full_output in enumerate(out_steered):\n",
    "            input_length = tok_batch['input_ids'][i].shape[0]\n",
    "            model_only_output = tokenizer.decode(full_output[input_length:])\n",
    "            model_only_output = model_only_output.replace(tokenizer.eos_token, '').replace('<|end_of_text|>', '')\n",
    "            model_only_decoded.append(model_only_output)\n",
    "        \n",
    "        model_only_responses.extend(model_only_decoded)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if (b_idx + 1) % backup_every == 0:\n",
    "            for i, global_idx in enumerate(batch_indices):\n",
    "                fname = os.path.splitext(filenames[global_idx])[0] + \"_steer_out.yaml\"\n",
    "                out_path = os.path.join(resdir, fname)\n",
    "\n",
    "                # Gather system prompt for this sample\n",
    "                sys_p = sys_prompt_batch if isinstance(sys_prompt_batch, str) else sys_prompt_batch[i]\n",
    "\n",
    "                data = {\n",
    "                    \"prompt\": prompt_batch[i],\n",
    "                    \"system_prompt\": sys_p,\n",
    "                    \"full_response\": full_decoded[i],\n",
    "                    \"model_only_response\": model_only_decoded[i],\n",
    "                }\n",
    "\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    yaml.safe_dump(data, f, allow_unicode=True, sort_keys=False)\n",
    "                    \n",
    "    return full_responses, model_only_responses, tok_batches, out_steered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50ada31f-c2a7-44f0-8b62-a0390216d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_steering_params(\n",
    "    layer_range: tuple[int, int],\n",
    "    num_layers: int,\n",
    "    effective_strength: float\n",
    ") -> tuple[list[int], float]:\n",
    "\n",
    "    # --- 1. Handle edge cases ---\n",
    "    assert num_layers <= layer_range[1] - layer_range[0]\n",
    "    \n",
    "    if num_layers <= 0:\n",
    "        return [], 0.0\n",
    "\n",
    "    # --- 2. Calculate the per-layer multiplier ---\n",
    "    multiplier = effective_strength / num_layers\n",
    "\n",
    "    # --- 3. Select evenly spaced layers ---\n",
    "    start_layer, end_layer = layer_range\n",
    "    \n",
    "    # If only one layer is requested, pick the middle of the range\n",
    "    if num_layers == 1:\n",
    "        middle_layer = round((start_layer + end_layer) / 2)\n",
    "        return [middle_layer], multiplier\n",
    "\n",
    "    # For multiple layers, calculate the step size to space them out evenly\n",
    "    # We divide by (num_layers - 1) to ensure the first and last points\n",
    "    # land on the start and end of the range.\n",
    "    step = (end_layer - start_layer) / (num_layers - 1)\n",
    "    \n",
    "    layers_to_steer = [\n",
    "        round(start_layer + i * step) for i in range(num_layers)\n",
    "    ]\n",
    "    \n",
    "    return layers_to_steer, multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47867168-1c29-493c-b1b1-03cfea50b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompts, user_prompts = load_lists_from_yaml(args.prompts_dir, args.mode)\n",
    "fnames = os.listdir(\"/pscratch/sd/r/ritesh11/temp/steering_experiments/steering_test_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85235321-c3e4-4c10-b762-14c6c98010fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_range = (15,18)\n",
    "num_layers = [2]\n",
    "strength = [2.5,3.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8a140aa-35e6-41b0-99fb-f688beef2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:24<03:43, 24.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m multiplier \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m steering_vecs \u001b[38;5;241m=\u001b[39m load_steering_vectors_from_npy(layers, multiplier\u001b[38;5;241m=\u001b[39mmultiplier, steering_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mvec_dir, \n\u001b[1;32m     14\u001b[0m                                                d_model\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39md_model,  model_len\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodel_len)\n\u001b[0;32m---> 16\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43msteer_and_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnnmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteering_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteering_vecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_to_steer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteer_on_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteer_on_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteer_on_thinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteer_on_thinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteer_on_system\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteer_on_system\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresdir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/N\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_S\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43ms\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 189\u001b[0m, in \u001b[0;36msteer_and_generate\u001b[0;34m(prompt_list, system_prompt, lma, tokenizer, steering_vectors, batch_size, max_new_tokens, temperature, layer_to_steer, d_model, steer_on_user, steer_on_thinking, steer_on_system, top_p, resdir, filenames, backup_every)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# First generation phase - generate up to 150 tokens with original logic\u001b[39;00m\n\u001b[1;32m    187\u001b[0m max_phase1_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m150\u001b[39m, max_new_tokens)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m lma\u001b[38;5;241m.\u001b[39mgenerate(tok_batch, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_phase1_tokens, temperature\u001b[38;5;241m=\u001b[39mtemperature, pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id, top_p\u001b[38;5;241m=\u001b[39mtop_p) \u001b[38;5;28;01mas\u001b[39;00m gen:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# Apply steering to specified layers\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_phase1_tokens):\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/intervention/contexts/interleaving.py:96\u001b[0m, in \u001b[0;36mInterleavingTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/contexts/tracer.py:25\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalTracingContext\n\u001b[1;32m     23\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mtry_deregister(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/contexts/base.py:72\u001b[0m, in \u001b[0;36mContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     69\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(graph\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], graph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 231\u001b[0m, in \u001b[0;36msteer_and_generate\u001b[0;34m(prompt_list, system_prompt, lma, tokenizer, steering_vectors, batch_size, max_new_tokens, temperature, layer_to_steer, d_model, steer_on_user, steer_on_thinking, steer_on_system, top_p, resdir, filenames, backup_every)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer_to_steer, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m layer_to_steer:\n\u001b[0;32m--> 231\u001b[0m         \u001b[43mapply_steering_to_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteering_vec_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_for_first_period\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m         layers[i]\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Single layer\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/r/ritesh11/temp/steering_experiments/drive_repo/utils.py:198\u001b[0m, in \u001b[0;36mapply_steering_to_layer\u001b[0;34m(layer_envoy, steering_vector, steering_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Apply steering only where mask is True\u001b[39;00m\n\u001b[1;32m    197\u001b[0m steering_expanded \u001b[38;5;241m=\u001b[39m steering_vector\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, 1, d_model)\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m layer_envoy\u001b[38;5;241m.\u001b[39moutput[:,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_envoy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m mask_expanded \u001b[38;5;241m*\u001b[39m steering_expanded\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/proxy.py:123\u001b[0m, in \u001b[0;36mProxy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[Self, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:250\u001b[0m, in \u001b[0;36mNode.create\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Otherwise just create the Node on the Graph like normal.\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/graph.py:131\u001b[0m, in \u001b[0;36mGraph.create\u001b[0;34m(self, target, redirect, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Redirection.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m redirect \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_class(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/intervention/graph/node.py:30\u001b[0m, in \u001b[0;36mInterventionNode.__init__\u001b[0;34m(self, fake_value, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, fake_value: Optional[Any] \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39m_empty, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     29\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfake_value \u001b[38;5;241m=\u001b[39m fake_value\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:63\u001b[0m, in \u001b[0;36mNode.__init__\u001b[0;34m(self, target, graph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremaining_dependencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecuted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# If theres an alive Graph, add it.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattached:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:408\u001b[0m, in \u001b[0;36mNode._meta_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback_str\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattached \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m--> 408\u001b[0m     meta_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meta_data\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/site-packages/nnsight/tracing/graph/node.py:391\u001b[0m, in \u001b[0;36mNode._meta_data.<locals>.traceback_str\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Compiles a string of all the lines in the Traceback up until nnsight code is called.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Str: Call Stack\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m traceback_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 391\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m stack:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# exclude frames created by nnsight or from the python environment\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch((\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/lib/python3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+/\u001b[39m\u001b[38;5;124m'\u001b[39m), frame\u001b[38;5;241m.\u001b[39mfilename)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nnsight/src/nnsight/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mfilename):\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    376\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 379\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/.conda/envs/myenv/lib/python3.10/linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(args.res_dir, exist_ok=True)\n",
    "\n",
    "for n in num_layers:\n",
    "    for s in strength: \n",
    "        layers, multiplier = calculate_steering_params(layer_range, n, s)\n",
    "\n",
    "        if args.mode == 'eval':\n",
    "            assert multiplier > 0\n",
    "        \n",
    "        if args.mode == 'deploy':\n",
    "            assert multiplier < 0\n",
    "            \n",
    "        steering_vecs = load_steering_vectors_from_npy(layers, multiplier=multiplier, steering_dir=args.vec_dir, \n",
    "                                                       d_model=args.d_model,  model_len=args.model_len)\n",
    "\n",
    "        ans = steer_and_generate(\n",
    "                prompt_list=user_prompts,\n",
    "                system_prompt=sys_prompts,\n",
    "                lma=nnmodel,\n",
    "                tokenizer=tokenizer,\n",
    "                steering_vectors=steering_vecs,\n",
    "                layer_to_steer=layers,\n",
    "                batch_size=args.batch_size,\n",
    "                max_new_tokens=args.max_new_tokens,\n",
    "                temperature=args.temperature,\n",
    "                d_model=args.d_model,\n",
    "                steer_on_user=args.steer_on_user,\n",
    "                steer_on_thinking=args.steer_on_thinking,\n",
    "                steer_on_system=args.steer_on_system,\n",
    "                top_p=args.top_p,\n",
    "                resdir = f\"{args.res_dir}/N{n}_S{s}\",\n",
    "                filenames=fnames,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e0e5d-58fb-4da3-bf71-a159a65170c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
